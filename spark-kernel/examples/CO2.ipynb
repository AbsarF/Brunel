{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-1.5.1-bin-hadoop2.3/lib/spark-assembly-1.5.1-hadoop2.3.0.jar!/org/apache/ivy/core/settings/ivysettings.xml:: resolving dependencies :: com.ibm.spark#spark-kernel;working\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-csv_2.10;1.3.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.3.0/spark-csv_2.10-1.3.0.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#spark-csv_2.10;1.3.0!spark-csv_2.10.jar (429ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-csv;1.1!commons-csv.jar (242ms)\n",
      "downloading https://repo1.maven.org/maven2/com/univocity/univocity-parsers/1.5.1/univocity-parsers-1.5.1.jar ...\n"
     ]
    }
   ],
   "source": [
    "%AddDeps com.databricks spark-csv_2.10 1.3.0 --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._ \n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "//Data is not aggregated\n",
    "val co2 = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Co2all.csv\")\n",
    "\n",
    "//Add a column that has Year as a true date type\n",
    "val convertDate = udf {\n",
    "    (year:Int) => year+ \"-01-01 \" +year + \" 00:00 UTC\"\n",
    "}\n",
    "val co2dates = co2.withColumn(\"year_as_date\", to_date(convertDate(co2(\"Year\"))) )\n",
    "\n",
    "//Data is aggregated by Spark\n",
    "val co2agg = co2.groupBy(\"CO2 per capita\").agg( avg(\"value\") as \"Mean Co2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Average CO2 emissions per Country\n",
    "Here, Spark does the aggegation.  Use the mouse wheel to zoom the map, click to pan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%brunel data('co2agg') map x(CO2_per_capita) color(Mean_Co2) tooltip(#all):: width=800, height=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Carbon emissions over time\n",
    "Click on a country in the map to see its CO2 emmissions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%brunel data('co2dates') \n",
    "        map('world') x(CO2_per_capita) color(value) mean(value) interaction(select) at(0,0,100,50) tooltip(#all) |\n",
    "        x(year_as_date) y(value) line label(CO2_per_capita) interaction(filter) at(0,50, 100,100) \n",
    ":: width=900, height=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.1 (Scala 2.10.4)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
